{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **THE CONTEXT**\n\nBefore we begin our project, we have to understand the data and the problem, what is the problem that we want to solve? is the data ready to process or we have to clean it? and so on. So let's break it down:\n\n**The Data**\n\nThe data came from The Flotation Plant. The data have several columns,the first column is date, the second and third columns are quality measures of the iron ore pulp right before it is fed into the flotation plant. Column 4 until column 8 are the most important variables that impact in the ore quality in the end of the process. From column 9 until column 22, we can see process data (level and air flow inside the flotation columns, which also impact in ore quality. The last two columns are the final iron ore pulp quality measurement from the lab. \n\n**The Problem**\n1. The aim is to predict the % Silica in Concentrate ever minute\n2. How many steps (hours) ahead can we predict % Silica in Concentrate?\n3. can we predict % Silica in Concentrate without using % Iron Concentrate?"},{"metadata":{},"cell_type":"markdown","source":"**DATA PREPROCESSING**\n\nThe first step in data science is data preprocessing. In this step,we have to:\n1. Handling Variable (Numerical, Categorical, Date/Time)\n2. Handling Missing Values\n3. Handling Outliers\n4. Scaling"},{"metadata":{},"cell_type":"markdown","source":"**IMPORTING LIBRARIES**\n\nimporting several libraries to process the data,\n* pandas for data manipulation\n* numpy for array processing \n* matplotlib for visualizatioin\n* seaborn for visualization but more advance in statistics"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LOAD DATA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we load the data, so we have to convert the date column into date and the others to float/integer\n# and we have to drop the duplicates entries/row using code below\n\ndata =  pd.read_csv('../input/quality-prediction-in-a-mining-process/MiningProcess_Flotation_Plant_Database.csv',\n                   decimal=\",\",\n                    parse_dates=[\"date\"],\n                    infer_datetime_format=True).drop_duplicates()\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the hape of data (row and column)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the data if there any missing value or not\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Display the data, and observe what kind of the data is these\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#in this case, we use heatmap to visualize the corealtion between each features\n\nplt.figure(figsize=(30, 30))\ncor= data.corr()\ncorelation = sns.heatmap(cor, annot=True, cmap=\"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pereparing Dataset For ML**\n\nAs you the result of code above, we saw there are several feature that  not really necessarily affect \ndepentdent feature, so we can drop the features and keep the most and more corelated independent features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop data that there are no significant corelation on dependent feature\n\n#data = data.drop(['date', \n             # '% Iron Concentrate', \n              #'Ore Pulp pH', \n              #'Flotation Column 01 Air Flow', \n              #'Flotation Column 02 Air Flow', \n              #'Flotation Column 03 Air Flow'], axis=1)\n\n#Correlation with output variable\ncor_target = abs(cor[\"% Silica Concentrate\"])\n\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.15]\nrelevant_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We pick the 3 biggest corelation\nrelevant_features = relevant_features.nlargest(n=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make a data from the releant features\ndata = pd.DataFrame(data, columns=relevant_features.index)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check The Distribution\nsns.distplot(data['Flotation Column 01 Air Flow'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check The Distribution\nsns.distplot(data['% Iron Concentrate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking The Outlier in our data\nsns.boxplot(data['Flotation Column 01 Air Flow'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking The Outlier in our data\nsns.boxplot(data['% Iron Concentrate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking The Outlier in our data\nsns.boxplot(data['% Silica Concentrate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the outlier with Percentiles\nfor i in data:\n    upper_lim = data[i].quantile(.95)\n    lower_lim = data[i].quantile(.05)\n\n    data = data[(data[i] < upper_lim) & (data[i] > lower_lim)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before we split into train and test data, as we can see, the data have differents in units and magnitude\n# So to make it at the same magnitude we can scaling the data\n\nY = data['% Silica Concentrate']\nX = data.drop(['% Silica Concentrate'], axis=1)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nX_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After we scaled the data, and the data have the same magnitude\n# we can split the data into Train & Test\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled,\n                                                    Y,\n                                                    test_size=0.3,\n                                                   random_state=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Making A Model**\n\nafter preprocessing step, we move to the model. In this step we have to figure the best model/algorithm that conclude the highest accuracy, but not overfitting or underfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nreg = LinearRegression()\nreg.fit(X_train, y_train)\n\ny_pred = reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**EVALUATING MODEL**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\n\nMSE = mean_squared_error(y_test, y_pred)\nprint('Our mean squared error is: ',MSE)\n\nMAE = mean_absolute_error(y_test, y_pred)\nprint('Our mean absolute error is: ',MAE)\n\nR2 = r2_score(y_test, y_pred) \nprint('Our R2 score is: ', R2)\n\nprint('Our Root Mean Squared Error is:', np.sqrt(mean_squared_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cecking Multicolinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\npd.DataFrame({'vif': vif[0:]}, index=X_train.columns).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PLoting the residual\nresidual = y_test - y_pred\nsns.distplot(residual)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy as sp\nfig, ax = plt.subplots(figsize=(6,2.5))\n_, (__, ___, r) = sp.stats.probplot(residual, plot=ax, fit=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking Homoscedacity\nsns.scatterplot(y_pred, residual)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check No Autocorelation Residua\nimport statsmodels.tsa.api as smt\n\nacf = smt.graphics.plot_acf(residual, lags=40 , alpha=0.05)\nacf.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize The Actual Data and our Prediction\nresult = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nresult.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15, 10))\nax = fig.add_subplot(111)\nax.scatter(y_test, y_pred)\nax.plot([0,max(y_test)], [0,max(y_pred)], color='r')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CONCLUSION ON THE LINEAR REGRESSION MODEL**"},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff_result = pd.DataFrame(reg.coef_, X.columns, columns=['Coefficient'])  \ncoeff_result","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}